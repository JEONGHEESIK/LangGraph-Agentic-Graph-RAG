# LangGraph-Agentic-Graph RAG

<div align="center">

<img src="https://img.shields.io/badge/python-3.10%2B-blue.svg" alt="Python">
<img src="https://img.shields.io/badge/Framework-LangGraph-orange.svg" alt="LangGraph"> 
<img src="https://img.shields.io/badge/Inference-SGLang-green.svg" alt="SGLang"> 
<img src="https://img.shields.io/badge/DB-Neo4j-blue.svg" alt="Neo4j"> 
<img src="https://img.shields.io/badge/DB-Weaviate-green.svg" alt="Weaviate"> 
<br>

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

English | [한국어](README/README_KO.md) | [中文](README/README_ZH.md)
</div> 

Traditional single-vector RAG systems face inherent limitations when dealing with complex, multi-hop queries, often remaining confined to passive information retrieval. To overcome this, I initially designed the Agentic Graph RAG—powered by LangGraph and SGLang—featuring an adaptive 3-way retrieval routing system to maximize reasoning accuracy and minimize unnecessary database traffic.

However, during the architectural design phase, I realized that by simply adding an intent-based Tool Routing layer and Model Context Protocol (MCP) on top of this robust retrieval foundation, it would be entirely possible to build a fully autonomous Agentic AI system.

With this high scalability in mind, the architecture was expanded. The resulting framework dynamically classifies user intent to seamlessly switch between deep knowledge retrieval and computational tasks (Calculator, SQL, APIs). Coupled with intelligent backtracking and Quality Gate validation, this project lays a highly scalable foundation for AI systems that not only retrieve information but actively think and execute tools.

LangGraph-Agentic-Graph RAG is a fully autonomous Agentic AI and vector–graph hybrid RAG platform powered by LangGraph + SGLang. The ingestion pipeline converts raw documents into Markdown chunks and graph metadata via LangGraph state machines with checkpoint persistence. During query time, an intent-based tool router dynamically delegates computational tasks to external tools (via MCP), while knowledge queries are routed through a hop-based router with quality-gate backtracking, selecting among three retrieval paths—Vector, Weaviate GraphRAG, or Neo4j GraphDB—to generate precise answers.

<div align="center">
<img src="https://github.com/user-attachments/assets/504ea0fa-ed9a-4664-9095-042e01debc65" width="512" height="768" ></img><br/>
</div> 

---

## Key Capabilities

- **LangGraph state machines**: ingestion, query reasoning, summarization, and mindmap generation all run on LangGraph `StateGraph` with `MemorySaver` checkpointing.
- **Checkpoint & backtracking**: every node transition is checkpointed; the quality gate and GoT snapshotting roll back to alternative retrieval paths or earlier expansions when quality is insufficient (configurable via `MAX_BACKTRACK_COUNT`).
- **3-way retrieval routing**: based on query complexity (hop count), routes to Vector RAG, Weaviate Cross-Reference GraphQL, or Neo4j Deep Graph Traversal (thresholds configurable via `GRAPH_MAX_HOPS`). Hop boundaries mirror the underlying data model: semantic-only questions stay on the TextDocument embedding index, entity/attribute hops leverage Weaviate cross-references, and schema-heavy relationship reasoning (≥ 6 hops) escalates to Neo4j Cypher traversals. Path 1 and Path 2 both live inside Weaviate: Path 1 favors fast semantic similarity on the late-chunked corpus, whereas Path 2 deliberately walks Cross-Reference neighborhoods to surface query-linked entities/events within Weaviate before escalating to Neo4j.
- **Observer LLM + GoT expansion**: an observer LLM scores each path result (0–1). Results above `QUALITY_GATE_THRESHOLD` trigger Graph-of-Thought branching/merging for deeper context.
- **SGLang inference**: generator, embedding, reranker, hop classifier, and the observer LLM all run on SGLang servers with lazy loading (`LazyModelManager` + `SGLangServerManager`).
  - **Lazy-loading**: servers auto-start on first request, idle timeout configurable via `SGLANG_IDLE_TIMEOUT` with GPU memory release
  - **GPU allocation**: Generator (device/mem_fraction configurable), Embedding/Reranker/Refiner (device/mem_fraction configurable)
  - **Chunk retry logic**: LLM metadata extraction auto-retries failed chunks with server restart (configurable via `GRAPH_EXTRACTOR_RETRY_ON_FAILURE`)
- **Automatic graph upsert**: OCR → LLM entity/event/relation extraction → simultaneous Weaviate + Neo4j ingestion.
- **Async job monitoring**: upload, OCR, and embedding progress tracked through task APIs.

---

## Architecture

```
┌──────────────┐    ┌─────────────────────────────┐
│ Input Layer  │ →  │ LangGraph Upload Pipeline   │ →  Markdown + *.graph.json
│ (PDF/IMG/…)  │    │ (MemorySaver checkpoint)    │
└──────────────┘    └─────────────────────────────┘
                                  │
┌──────────────┐    ┌─────────────────────────────┐
│ User Query   │ →  │ LangGraph RAG Workflow      │
└──────────────┘    │ (MemorySaver checkpoint)    │
                    └─────────────────────────────┘
                                  │
                    ┌─────────────┴──────────────┐
                    │  Tool Router (LLM intent)  │
                    │  (ToolExecutor.classify)   │
                    └─────────────┬──────────────┘
                                  │
                    ┌─────────────┴──────────────┐
                    │                            │
              Knowledge Query              Computational Task
                    │                            │
        ┌───────────┴──────────────┐        ┌───────┴────────┐
        │   RAG Router (reasoner)  │        │ Tool Executor  │
        │ HopClassifier+PathSelect │        │ (MCP/Local)    │
        └───────────┬──────────────┘        └────────────────┘
                    │
     ┌──────────────┼──────────────┐
     │              │              │
 Path 1        Path 2         Path 3
 Vector        CrossRef       GraphDB
 Search        Weaviate Ref   Neo4j Cypher
 (≤ 2 hop)     (3–5 hop)      (≥ 6 hop)
                    │
                    ▼
   ┌───────────────────────────────────────┐
   │ Quality Gate (QualityEvaluator)       │
   │ Observer LLM (QUALITY_GATE_THRESHOLD) │
   └────────────────┬──────────────────────┘
                    │
          ┌─────────▼──────────────┐
          │ GoT Thought Expander   │
          │ Branch merge + pruning │
          └─────────┬──────────────┘
                    │
             ┌──────▼──────┐
             │ LLM Answer  │
             └─────────────┘
```

### Core Components

**Reasoner Module** (`backend/notebooklm/reasoner/`):
- `HopClassifier`: Query complexity estimation (LLM + heuristic fallback)
- `PathSelector`: Optimal retrieval path selection for backtracking
- `QualityEvaluator`: Observer LLM-based result quality assessment
- `VectorRetriever`, `CrossRefRetriever`, `GraphDBRetriever`: Modular retrieval implementations

### Workflow Graph (Query)

```
planner → tool_router ┬→ rag_router →┬→ vector_retriever  ──→┐
                      │               ├→ crossref_retriever ─→├→ quality_gate →┬→ thought_expander → aggregator → END
                      │               └→ graphdb_retriever ──→┘                └→ rag_router (backtrack)
                      │
                      └→ tool_executor ────────────────────────────────────────→ aggregator → END
```

**Tool Router**: Classifies query intent (knowledge/calculation/database/api_call/code_exec) using LLM (configurable via `TOOL_INTENT_CLASSIFIER_ENDPOINT`). Routes to RAG pipeline for knowledge queries or to tool executor for computational tasks.

**Tool Executor** (`backend/notebooklm/tools/`):
- MCP server integration (if `MCP_SERVER_ENABLED=true`) or local fallback
- Intent classification: LLM-based with heuristic fallback
- Currently supports:
- Calculator (AST-based safe evaluation)
- SQL executor (stub)
- API caller (stub)
- Code runner (stub)

---

## Input / Preprocessing Layer

Handled by `LangGraphUploadPipeline` (`langgraph_upload_pipeline.py`) with `MemorySaver` checkpointing across all nodes:

1. **Conversion & Layout**: `run_file_processor.py` handles PDF/Office/image/audio inputs → `Results/1.Converted_images` + `Results/2.LayoutDetection`.
2. **OCR & Markdown**: `run_ocr_processing()` with SGLang-powered OCRFlux produces per-page Markdown → `Results/4.OCR_results`.
3. **LLM Metadata Extraction**: `LLMMetadataExtractor` extracts entities/events/relations from Markdown → `Results/8.graph_metadata/*.graph.json`.
   - **Chunk size**: configurable via `GRAPH_EXTRACTOR_CHUNK_SIZE`
   - **Timeout**: configurable via `GRAPH_EXTRACTOR_API_TIMEOUT`
   - **Retry logic**: On timeout, SGLang generator server restarts and retries the same chunk once
   - **Keepalive**: Background thread touches server every `SGLANG_KEEPALIVE_INTERVAL` seconds during processing
4. **Graph Upsert**:
   - `GraphSchemaManager` ensures Weaviate GraphEntity/GraphEvent/GraphRelation collections exist with cross-references (source/target/event)
   - `LegacyGraphIngestor` / `Neo4jManager` MERGEs nodes/relationships into Neo4j with deterministic UUIDs
5. **Late Chunking & Embedding**: `embedding_text.py` splits Markdown into chunks and uploads into the Weaviate TextDocument collection via `SharedEmbeddingModel` (model configurable via `EMBEDDING_MODEL`).

---

## Query / Reasoning Layer

Handled by `GraphReasoner` (`graph_reasoner.py`) with `MemorySaver` checkpointing and intelligent backtracking:

1. **Hop Classifier (LLM + Heuristic Hybrid)** (`HopClassifier`): 
   - **Primary**: LLM-based classifier estimates query complexity (1–`GRAPH_MAX_HOPS`) via SGLang server.
   - **Fallback**: Heuristic scoring based on keywords, arrow count, and concept separators.
   - The estimated hop count determines the initial retrieval path.
2. **Planner**: analyzes the query, sets `max_hops` dynamically based on LLM+Heuristic classification (capped by `GRAPH_MAX_HOPS`), and initializes the search plan.
3. **Router**: selects Path 1/2/3 based on hop classification and query characteristics.
   - **Initial routing**: hop ≤ 2 → Path 1 (VectorRetriever), 3–5 → Path 2 (CrossRefRetriever), ≥ 6 → Path 3 (GraphDBRetriever).
   - **Backtracking routing**: evaluates remaining untried paths using `PathSelector.select_best_path()`, which scores each path based on query keywords and hop count to select the most suitable alternative.
4. **Retrieval Nodes**:
   - **Path 1 – Vector RAG**: `rag_pipeline`'s TextSearcher + Reranker.
   - **Path 2 – Cross-Ref GraphRAG**: BM25 seed entities → Weaviate Cross-Reference `QueryReference` multi-hop traversal (source/target/event refs) to collect query-adjacent entities/events within Weaviate before escalating to Neo4j.
   - **Path 3 – Neo4j GraphDB**: deep traversal via `legacy_graph_client.py` Cypher templates for ≥ 6-hop schema-intensive reasoning or when Weaviate cross-references exhaust.
5. **Quality Gate + Observer LLM**: every path result is scored 0.0–1.0 by the observer LLM (Path 1 defaults to 1.0 when delegated). 
   - **Pass condition**: quality ≥ `QUALITY_GATE_THRESHOLD` → proceeds to next stage.
   - **Backtracking**: quality < `QUALITY_GATE_THRESHOLD` → `PathSelector.select_best_path()` evaluates remaining paths based on query characteristics (keywords, hop count) and selects the most suitable alternative path.
   - **Termination**: after all paths are exhausted or `MAX_BACKTRACK_COUNT` limit is reached, continues with best-effort context and flags degradation in `answer_notes`.
   - **Path tracking**: `tried_paths` state field prevents re-trying the same path.
6. **GoT Thought Expander** (`GOT_MODE_ENABLED=true`): graph-shaped thought exploration with branching, scoring, and merging.
   - Each step fans out up to `GOT_BRANCH_FACTOR` candidate queries in parallel.
   - An observer LLM scores each branch (0.0–1.0) for relevance, coverage, and novelty.
   - Branches above `GOT_THOUGHT_SCORE_THRESHOLD` are merged via `GOT_MERGE_STRATEGY` (`top_k` / `weighted_union` / `vote`).
   - Low-quality edges are pruned by `GOT_EDGE_PRUNE_THRESHOLD` keyword-overlap scoring.
   - Consecutive all-branch failures (tracked by `GOT_MAX_CONSECUTIVE_FAILURES`) trigger snapshot-based backtracking (state rollback to last successful merge).
7. **Aggregator**: builds context snippets from entities/events/relations for LLM generation.
8. **Generation** (`generator.py`): merges thought/path snippets with the original query to produce the answer; `refiner.py` / `evaluator.py` optionally post-process the response.

---

## Module Map

```
backend/
├── main.py                          # [Excluded] FastAPI server entry point
├── config.py                        # [Excluded] Server-level configuration
├── logging_config.py                # [Excluded] Logging configuration
│
├── api/                             # API layer
│   ├── routes.py                    # [Excluded] Main upload/file/session routes
│   ├── chat.py                      # [Excluded] POST /v1/chat endpoint
│   ├── ocr_routes.py                # [Excluded] OCR processing endpoints
│   └── pause_api.py                 # [Excluded] Task pause/resume API
│
├── notebooklm/                      # RAG core modules
│   ├── config.py                    # Model/path/graph configuration
│   ├── rag_pipeline.py              # LangGraph RAG workflow orchestrator
│   ├── graph_reasoner.py            # LangGraph workflow orchestration
│   ├── graph_schema.py              # Weaviate Entity/Event/Relation schema
│   ├── hop_classifier.py            # Query complexity estimator
│   ├── reasoner/                    # Refactored GraphReasoner modules
│   │   ├── state.py                 # GraphReasonerState definition
│   │   ├── routing.py               # PathSelector, HopClassifier
│   │   ├── quality.py               # QualityEvaluator
│   │   ├── retrievers.py            # VectorRetriever, CrossRefRetriever, GraphDBRetriever
│   │   └── __init__.py
│   ├── legacy_graph_client.py       # Neo4j Cypher traversal client
│   ├── legacy_graph_ingestor.py     # Neo4j upsert helper
│   ├── embedding_text.py            # Late chunking + Weaviate text indexing
│   ├── embedding_image.py           # Image embedding + Weaviate image indexing
│   ├── image_processor.py           # Image processing utilities
│   ├── shared_embedding.py          # SGLang embedding/reranker client (singleton)
│   ├── sglang_server_manager.py     # SGLang server lifecycle manager
│   ├── generator.py                 # LLM answer generation
│   ├── refiner.py                   # Answer refinement
│   ├── evaluator.py                 # Answer quality evaluation
│   ├── router.py                    # Query type routing
│   ├── query_rewriter.py            # Query rewriting
│   ├── parallel_search.py           # Parallel text+image search
│   ├── weaviate_utils.py            # Weaviate client utilities
│   ├── clean_weaviate.py            # [Excluded] Weaviate + Neo4j data cleanup script
│   ├── tools/                       # Tool calling & MCP integration
│   │   ├── mcp_client.py            # MCP server REST client
│   │   ├── tool_executor.py         # Tool execution (MCP/local fallback)
│   │   └── __init__.py
│   ├── rag_text/                    # Text search + reranker
│   └── rag_image/                   # Image search + reranker
│
├── mcp_server/                      # MCP (Model Context Protocol) Tool Server
│   └── main.py                      # FastAPI server for tool execution
│
├── data_pipeline/                    # Data processing pipeline
│   └── pipe/
│       ├── langgraph_upload_pipeline.py  # LangGraph upload workflow (checkpointed)
│       ├── llm_metadata_extractor.py     # Entity/event/relation extraction
│       ├── neo4j_manager.py              # Neo4j upsert manager
│       ├── run_file_processor.py         # Convert/layout/OCR orchestrator
│       ├── pipeline_image.py             # Image pipeline
│       ├── pipeline_sound.py             # Audio pipeline
│       └── main_pipe/
│           ├── ocr_pipe/                 # SGLang-based OCRFlux engine
│           ├── udp_pdftopng_300dpi.py    # PDF → PNG conversion
│           └── udp_layoutdetection.py    # Layout detection
│
├── services/                        # Business logic services
│   ├── model_manager.py             # [Excluded] LazyModelManager (GPU lifecycle)
│   ├── ocr_vision_manager.py        # [Excluded] OCR engine management
│   └── rag_service.py               # [Excluded] RAG service orchestration
│
└── utils/
    ├── task_queue.py                # [Excluded] GPU task queue (async job management)
    ├── helpers.py                   # [Excluded] Shared utility functions
    ├── path_helpers.py              # [Excluded] Path calculation helpers
    └── file_utils.py                # [Excluded] File operation utilities
```

---

## Data Pipeline

1. **File upload** (`POST /upload/files`)
   - `api/routes.py` stores files in per-session folders and enqueues `run_processing_pipeline` via `task_queue.py`.
2. **GPU task queue**
   - `task_queue.py` manages sequential GPU-bound tasks (convert → layout → OCR) with progress tracking.
3. **Text indexing** (`run_text_indexing` / `run_text_indexing_v2`)
   - Initializes `SharedEmbeddingModel` → runs `process_markdown_files` for Weaviate late-chunking indexing.
4. **Graph extraction & ingestion**
   - `LLMMetadataExtractor` produces `*.graph.json` → `GraphSchemaManager` upserts to Weaviate → `Neo4jManager` / `LegacyGraphIngestor` upserts to Neo4j.
5. **Storage state**
   - Weaviate: TextDocument + GraphEntity/Event/Relation collections.
   - Neo4j: Entity/Event nodes + relation edges with deterministic UUIDs and auto-created constraints.

---

## Query Processing Flow

1. `POST /v1/chat` → `rag_pipeline.retrieve()` is invoked.
2. `graph_reasoner.py` runs the LangGraph workflow:
   - **planner** → analyzes query, sets `max_hops` based on LLM+Heuristic hop classification.
   - **tool_router** → LLM classifies query intent (knowledge/calculation/database/api_call/code_exec):
     - If `knowledge` → routes to **rag_router**
     - If computational task → routes to **tool_executor**
   - **rag_router** (for knowledge queries) → selects initial path (Vector/Cross-Ref/GraphDB) based on hop count:
     - hop ≤ 2 → `vector_retriever`
     - hop 3-5 → `crossref_retriever`
     - hop ≥ 6 → `graphdb_retriever`
   - **tool_executor** (for computational tasks) → executes tool via MCP server or local fallback:
     - Calculator: AST-based safe math evaluation
     - SQL/API/Code: stub implementations (extensible via MCP)
   - **retrieval node** (Path 1/2/3) → executes selected retrieval strategy.
   - **quality_gate** → Observer LLM scores result (0.0–1.0).
     - If quality < `QUALITY_GATE_THRESHOLD` → **intelligent backtracking**: `_select_best_path()` evaluates remaining paths based on query keywords and hop count, selects the most suitable alternative (max `MAX_BACKTRACK_COUNT` retries).
   - If GoT enabled → **thought_expander** with snapshot-based backtracking.
   - **aggregator** → builds context snippets (for RAG) or formats tool result.
3. `generator.py` produces the answer from original query + context snippets (or tool result).
4. (Optional) `refiner.py` polishes the answer; `evaluator.py` logs quality notes.
5. Response includes `plan`, `hops`, `notes`, `context_snippets`, `backtrack_count`, `tried_paths`, `thought_steps`, and `tool_result` (if applicable) for debugging.

---

## Installation & Run

```bash
cd backend
pip install -r requirements.txt

# Environment variables (see .env.example)
export GRAPH_RAG_ENABLED=true
export LANGGRAPH_ENABLED=true
export GOT_MODE_ENABLED=true
export LEGACY_GRAPH_ENABLED=true
export NEO4J_URI=bolt://localhost:7687
export NEO4J_USER=neo4j
export NEO4J_PASSWORD=your_password

# LLM Models (customize based on your hardware)
export LLM_MODEL
export EMBEDDING_MODEL
export RERANKER_MODEL

# SGLang server endpoints
export SGLANG_GENERATOR_ENDPOINT=http://localhost:30000
export SGLANG_EMBEDDING_ENDPOINT=http://localhost:30001
export SGLANG_RERANKER_ENDPOINT=http://localhost:30002

# Graph extractor settings
export GRAPH_EXTRACTOR_API_TIMEOUT
export GRAPH_EXTRACTOR_CHUNK_SIZE
export GRAPH_EXTRACTOR_RETRY_ON_FAILURE=true

python main.py
# SGLang servers (generator/embedding/reranker) autostart on first request with lazy-loading.
```

To launch Neo4j locally:

```bash
docker run -d --name neo4j-dev \
  -p7474:7474 -p7687:7687 \
  -e NEO4J_AUTH=neo4j/your_password \
  -v neo4j-data:/data \
  neo4j:5
```

---

## Key Settings (`notebooklm/config.py`)

- **Graph RAG toggles**: `GRAPH_RAG_ENABLED`, `LANGGRAPH_ENABLED`, `GOT_MODE_ENABLED`, `GRAPH_MAX_HOPS`.
- **GoT tuning**: `GOT_BRANCH_FACTOR`, `GOT_MERGE_STRATEGY` (`top_k`/`weighted_union`/`vote`), `GOT_MERGE_TOP_K`, `GOT_THOUGHT_SCORE_THRESHOLD`, `GOT_EDGE_PRUNE_THRESHOLD`, `GOT_MAX_STEPS`, `GOT_MAX_CONSECUTIVE_FAILURES`, `GOT_OBSERVER_ENDPOINT`/`GOT_OBSERVER_MODEL`.
- **Neo4j**: `NEO4J_URI`, `NEO4J_USER`, `NEO4J_PASSWORD`, `GRAPH_MAX_HOPS`.
- **Weaviate**: `WEAVIATE_HOST/PORT`, `WEAVIATE_TEXT_CLASS` (TextDocument), `WEAVIATE_VECTORIZER` (text2vec-model2vec).
- **SGLang models**: `LLM_MODEL`, `EMBEDDING_MODEL`, `RERANKER_MODEL`, `REFINER_MODEL`, `QUERY_REWRITER_MODEL`.
- **SGLang servers**: `SGLANG_GENERATOR_ENDPOINT`, `SGLANG_EMBEDDING_ENDPOINT`, `SGLANG_RERANKER_ENDPOINT`, `SGLANG_REFINER_ENDPOINT`, `SGLANG_QUERY_REWRITER_ENDPOINT`.
- **SGLang lifecycle**: `SGLANG_IDLE_TIMEOUT` (60s), `SGLANG_KEEPALIVE_INTERVAL` (20s).
- **Graph extractor**: `GRAPH_EXTRACTOR_API_TIMEOUT` (60s), `GRAPH_EXTRACTOR_CHUNK_SIZE` (800), `GRAPH_EXTRACTOR_RETRY_ON_FAILURE` (true).
- **Session directories**: `DATA_ROOT/Results`, `sessions/<id>` layout.

---

## MCP Tool Server

The MCP (Model Context Protocol) server is an independent FastAPI service that handles tool execution for the LangGraph RAG system. It provides a REST API for executing computational tools like calculators, SQL queries, API calls, and code execution.

### Features

- **Calculator**: AST-based safe mathematical expression evaluation
- **SQL Executor**: SQL query execution (stub implementation, extensible)
- **API Caller**: External API invocation (stub implementation, extensible)
- **Code Runner**: Code execution sandbox (stub implementation, extensible)

### Installation & Run

```bash
cd mcp_server
pip install -r requirements.txt

# Start server (default port: 8001)
python main.py

# Or use uvicorn directly
uvicorn main:app --host 0.0.0.0 --port 8001 --reload
```

### Configuration

Set the following in your main RAG system's `config.py`:

```python
MCP_SERVER_ENABLED = True
MCP_SERVER_URL = "http://localhost:8001"
MCP_TIMEOUT = 30
```

### API Endpoints

| Endpoint | Method | Description |
|---|---|---|
| `/health` | GET | Health check |
| `/tools` | GET | List available tools |
| `/tools/{tool_name}/execute` | POST | Execute a specific tool |

### Example Usage

```bash
# Calculator example
curl -X POST http://localhost:8001/tools/calculator/execute \
  -H "Content-Type: application/json" \
  -d '{"inputs": {"expression": "10 + 20 * 3"}}'

# Response:
# {
#   "status": "ok",
#   "result": 70.0,
#   "message": "10 + 20 * 3 = 70.0"
# }
```

### Adding New Tools

1. Implement executor function in `main.py`
2. Register in `TOOL_REGISTRY`
3. Restart server

```python
def execute_my_tool(inputs: Dict[str, Any]) -> ToolExecuteResponse:
    # Implementation
    return ToolExecuteResponse(status="ok", result=...)

TOOL_REGISTRY["my_tool"] = {
    "executor": execute_my_tool,
    "info": ToolInfo(
        name="my_tool",
        description="Description of my tool",
        parameters={...}
    )
}
```

---

## API Highlights

| Endpoint | Description |
|---|---|
| `POST /upload/files` | Triggers the LangGraph upload pipeline |
| `POST /v1/chat` | Runs 3-way RAG with checkpoint/backtracking |
| `GET /api/v1/tasks/{task_id}` | Monitors queued upload/OCR tasks |
| `GET /files` | Lists session artifacts |
| `POST /pause` | Pauses/resumes background tasks |

---

## Logging & Operations

- `sglang_embedding_server.log`, `sglang_reranker_server.log` – SGLang model server health.
- `Results/8.graph_metadata/*.graph.json` – archive of LLM extraction results.
- `LegacyGraphIngestor` auto-creates constraints on first run; no manual setup required.
- `SGLangServerManager` releases GPU memory after 60 seconds of idling (configurable via `SGLANG_IDLE_TIMEOUT`).
- All LangGraph workflows log checkpoint IDs and backtrack counts for traceability.
- **SGLang cold start**: lazy loading means the first `/v1/chat` (or hop-classifier) request must warm each SGLang server, which can take 20–60s VRAM load time; issue a warm-up request or keep-alive cron to avoid client timeouts.
- **Chunk retry mechanism**: If LLM metadata extraction times out (default 60s), the generator server is automatically restarted and the same chunk is retried once. This prevents hanging on problematic chunks while maintaining extraction quality.
- **Volatile checkpoints**: `MemorySaver` stores graph snapshots in-process, so any FastAPI restart drops in-flight state until the planned migration to `SqliteSaver`/`PostgresSaver` lands.
- **Weaviate v4 API**: Uses `weaviate.connect_to_custom()` with gRPC support (port 50051).

---

## Roadmap

1. ~~**GoT (Graph of Thought)**~~
   - `thought_expander` now performs graph-shaped exploration: each step fans out `GOT_BRANCH_FACTOR` branches, an observer LLM scores each branch, and the best results are merged via `GOT_MERGE_STRATEGY`. Low-quality edges are pruned, and consecutive failures trigger snapshot-based backtracking.
2. **Advanced hop classifier**
   - Augment with query metadata (token length, entity counts) for a hybrid router.
3. **Multi-graph retrieval optimization**
   - Improve context filtering/dedup for 3–5 hop Weaviate traversals and add Cypher templates for ≥ 6 hop Neo4j exploration.
4. **LangGraph workflow observability**
   - Emit per-node latency/error metrics and integrate retry policies inside `GraphReasoner` and `LegacyGraphClient`.
5. **Persistent checkpointer**
   - Migrate from `MemorySaver` to `SqliteSaver` / `PostgresSaver` for cross-session state recovery.

---

## Contribution & Contact

Issues and PRs are welcome. For questions or concerns, please open an issue on GitHub or email us at **koto144@gmail.com**.

---

## License

This project is dual-licensed under:
- **MIT License** - see the [LICENSE](LICENSE) file for details
- **Apache License 2.0** - see the [LICENSE-APACHE](LICENSE-APACHE) file for details

You may choose either license to govern your use of this software.

---

## Citation

If you use this project in your research, please cite the following:

### SGLang
```bibtex
@misc{zheng2023sglang,
  title={SGLang: Efficient Execution of Structured Language Model Programs},
  author={Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Jeff Huang and Chuyue Sun and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},
  year={2023},
  url={https://github.com/sgl-project/sglang}
}
```

### LangGraph
```bibtex
@software{langgraph2024,
  title={LangGraph: A Framework for Building Stateful Multi-Actor Applications},
  author={LangChain AI},
  year={2024},
  url={https://github.com/langchain-ai/langgraph}
}
```

### Weaviate
```bibtex
@software{weaviate2024,
  title={Weaviate: An Open-Source Vector Database},
  author={Weaviate B.V.},
  year={2024},
  url={https://github.com/weaviate/weaviate}
}
```

### Neo4j
```bibtex
@software{neo4j2024,
  title={Neo4j: The Graph Database Platform},
  author={Neo4j, Inc.},
  year={2024},
  url={https://github.com/neo4j/neo4j}
}
```

### OCRFlux
```bibtex
@software{ocrflux2024,
  title={OCRFlux: Vision-Language Model for OCR},
  author={ChatDOC},
  year={2024},
  url={https://huggingface.co/ChatDOC/OCRFlux-3B}
}
```